---
layout: post
title: What can we learn from neuroscience?
excerpt:
---

I work at the [MIT Department of Brain and Cognitive Sciences][4], in a big building on Vassar St. in Cambridge, Massachusetts. This building is an extremely diverse place: under the same roof are biologists studying signaling mechanisms in the brains of monkeys and [zebrafish][1], experimental psychologists evaluating how human children solve problems, and computationalists building broad theories about the algorithms of the mind.

It's a dizzyingly interdisciplinary place.[^1] While everyone here is  supposedly chasing the same target — [to "reverse-engineer the human mind"][2] — it's often tough to understand how we could all be on the same team with such different methods. One of the most challenging and important questions we ask each other in this building is *why:* Why is your work relevant to that mission of "reverse-engineering the human mind?"

This question has been especially relevant over the past few months as I spend more time around the cognitive neuroscientists in this building — those researchers who look to the brain in order to explain the architecture and algorithms of the human mind.

More directly: as a cognitive scientist, I'm still trying to figure out what the "neuro" part of "cognitive neuroscientist" adds to the picture. This post will share a brief update on that journey, largely thanks to discussions in the [MIT Brain and Cognitive Sciences Philosophy Circle][3].

## Why cognitive neuroscience?

Cognitive neuroscience attempts to explain how the brain actually implements TODO

### A new tool for testing cognitive theories

Some of the most exciting recent cognitive neuroscience directly links the representational constructs of sophisticated cognitive theories with activity in the brain. [Nelson et al. (2017)][nelson2017neurophysiological] attempt to show, for example, how brain activity recorded while human subjects read English sentences actually replicates the sorts of signals we'd expect to find in [cognitively motivated models of incremental syntactic parsing][hale2010what]. TODO another example

These studies use brain activity as a tool to arbitrate between alternative cognitive theories about how the mind computes. Here, then, the signals recorded from the brain simply help us *select* among existing cognitive theories.

Lots of different experiments can help us select among existing cognitive theories, though. We have lots of behavioral measures to test how people comprehend language, for example: we might look at [how their eyes move across a visual scene as they hear spoken language][altmann1999incremental], or TODO another example.

[todo Marr diagram here? show top-down vs. bottom-up]

From this perspective, brain signals seem like one of just many tools we might use to test our cognitive theories.

### But brains are hard to understand

- noisy (requires an entirely new field of applied statistics)
- expensive
- huge assumptions required about e.g. modularity, processing architecture, neuron doctrine, etc. in order to even get off the ground

so why ever use such an expensive tool when we have more proximal, more economical tools?

### Bottom-up constraints

can provide bottom-up constraints on computation [see examples in work notebook, incl. feedforward nature of ventral visual stream]

[1]: https://en.wikipedia.org/wiki/Zebrafish#In_scientific_research
[2]: http://bcs.mit.edu/about-bcs
[3]: https://sites.google.com/view/mit-bcs-philosophy/home
[4]: http://bcs.mit.edu/
[nelson2017physiological]: http://www.pnas.org/content/114/18/E3669
[hale2010what]: https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1551-6709.2010.01145.x
[altmann1999incremental]: https://www.ncbi.nlm.nih.gov/pubmed/10585516

[^1]: If you fit anywhere on the spectrum from biology to computer science and are interested in discovering the mysteries of the mind/brain, you might consider [joining us](http://bcs.mit.edu/academic-program/graduate/graduate-admissions)!